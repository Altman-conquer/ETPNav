from math import pi, tan, cos, sin

import cv2
import numpy as np
import open3d as o3d
from matplotlib import pyplot as plt
from numpy import linalg as LA

from vlnce_baselines.map_navigation.map_utils import find_first_nonzero_elem_per_row, apply_color_to_map


class semantic_map_habitat_tools:
    """ class used to build semantic maps of the scenes.
    It takes dense observations of the environment and project pixels to the ground.
    """

    def __init__(self, saved_folder):
        from core import cfg

        self.scene_name = ''
        self.cell_size = cfg.SEM_MAP.CELL_SIZE
        self.step_size = 1000
        self.map_boundary = 5
        self.detector = None
        self.saved_folder = saved_folder

        self.IGNORED_CLASS = [0, 1]  # ceiling class is ignored

        # ==================================== initialize 4d grid =================================
        self.min_X = -cfg.SEM_MAP.WORLD_SIZE
        self.max_X = cfg.SEM_MAP.WORLD_SIZE
        self.min_Z = -cfg.SEM_MAP.WORLD_SIZE
        self.max_Z = cfg.SEM_MAP.WORLD_SIZE
        self.min_Y = 0.0
        self.max_Y = cfg.SENSOR.AGENT_HEIGHT + self.cell_size

        self.x_grid = np.arange(self.min_X, self.max_X, self.cell_size)
        self.z_grid = np.arange(self.min_Z, self.max_Z, self.cell_size)
        self.y_grid = np.arange(self.min_Y, self.max_Y, self.cell_size)

        self.THRESHOLD_HIGH = len(self.y_grid)

        self.four_dim_grid = np.zeros(
            (len(self.z_grid), len(self.y_grid) + 1,
             len(self.x_grid), cfg.SEM_MAP.GRID_CLASS_SIZE),
            dtype=np.int16)  # x, y, z, C

        # ===================================
        self.H, self.W = len(self.z_grid), len(self.x_grid)
        self.min_x_coord = self.W - 1
        self.max_x_coord = 0
        self.min_z_coord = self.H - 1
        self.max_z_coord = 0
        self.max_y_coord = 0

        self.pcd = o3d.geometry.PointCloud()  # 初始化为 None，后续可赋值为 open3d.geometry.PointCloud()

    def convert_insseg_to_sseg(self, insseg, ins2cat_dict):
        """
        convert instance segmentation image InsSeg (generated by Habitat Simulator) into Semantic segmentation image SSeg,
        given the mapping from instance to category ins2cat_dict.
        """
        ins_id_list = list(ins2cat_dict.keys())
        sseg = np.zeros(insseg.shape, dtype=np.int16)
        for ins_id in ins_id_list:
            sseg = np.where(insseg == ins_id, ins2cat_dict[ins_id], sseg)
        return sseg

    def project_semantic_pixels_to_world_coords(sseg_img,
                                                current_depth,
                                                current_pose,
                                                gap=2,
                                                FOV=79,
                                                cx=320,
                                                cy=240,
                                                theta_x=0.0,
                                                resolution_x=640,
                                                resolution_y=480,
                                                ignored_classes=[],
                                                sensor_height=1.5):
        """
        Project pixels in sseg_img into world frame given depth image current_depth and camera pose current_pose.
        (u, v) = KRT(XYZ)
        """
        from core import cfg

        # camera intrinsic matrix
        radian = FOV * pi / 180.
        focal_length = cx / tan(radian / 2)
        K = np.array([[focal_length, 0, cx], [0, focal_length, cy], [0, 0, 1]])
        inv_K = LA.inv(K)
        # first compute the rotation and translation from current frame to goal frame
        # then compute the transformation matrix from goal frame to current frame
        # thransformation matrix is the camera2's extrinsic matrix
        tx, tz, theta = current_pose

        R_y = np.array([[cos(theta), 0, sin(theta)], [0, 1, 0],
                        [-sin(theta), 0, cos(theta)]])
        # used when I tilt the camera up/down
        R_x = np.array([[1, 0, 0], [0, cos(theta_x), -sin(theta_x)],
                        [0, sin(theta_x), cos(theta_x)]])
        R = R_y.dot(R_x)
        T = np.array([tx, 0, tz])
        transformation_matrix = np.empty((3, 4))
        transformation_matrix[:3, :3] = R
        transformation_matrix[:3, 3] = T

        # build the point matrix
        x = range(0, resolution_x, gap)
        y = range(0, resolution_y, gap)
        xv, yv = np.meshgrid(np.array(x), np.array(y))
        Z = current_depth[yv.flatten(),
        xv.flatten()].reshape(yv.shape[0], yv.shape[1])
        points_4d = np.ones((yv.shape[0], yv.shape[1], 4), np.float32)
        points_4d[:, :, 0] = xv
        points_4d[:, :, 1] = yv
        points_4d[:, :, 2] = Z
        points_4d = np.transpose(points_4d, (2, 0, 1)).reshape((4, -1))  # 4 x N

        # apply intrinsic matrix
        points_4d[[0, 1, 3], :] = inv_K.dot(points_4d[[0, 1, 3], :])
        points_4d[0, :] = points_4d[0, :] * points_4d[2, :]
        points_4d[1, :] = points_4d[1, :] * points_4d[2, :]

        # transform kp1_4d from camera1(current) frame to camera2(goal) frame through transformation matrix
        points_3d = transformation_matrix.dot(points_4d)

        # reverse y-dim and add sensor height
        points_3d[1, :] = points_3d[1, :] * -1 + sensor_height

        # ignore some artifacts points with depth == 0
        depth_points = current_depth[yv.flatten(), xv.flatten()].flatten()
        good = np.logical_and(depth_points > cfg.SENSOR.DEPTH_MIN,
                              depth_points < cfg.SENSOR.DEPTH_MAX)

        points_3d = points_3d[:, good]

        # pick x-row and z-row
        sseg_points = sseg_img[yv.flatten(), xv.flatten()].flatten()
        sseg_points = sseg_points[good]

        # ignore some classes points
        for c in ignored_classes:
            good = (sseg_points != c)
            sseg_points = sseg_points[good]
            points_3d = points_3d[:, good]

        return points_3d, sseg_points.astype(int)

    def build_semantic_map(self, rgb_img, depth_img, insseg_img, pose, step_):
        """ update semantic map with observations rgb_img, depth_img, sseg_img and robot pose."""
        global ins2cat_dict

        sem_map_pose = (pose[0], -pose[1], -pose[2])  # x, z, theta

        sseg_img = self.convert_insseg_to_sseg(insseg_img, ins2cat_dict)

        xyz_points, sseg_points = self.project_semantic_pixels_to_world_coords(
            sseg_img, depth_img, sem_map_pose, gap=2, FOV=90, cx=128, cy=128, resolution_x=256, resolution_y=256,
            ignored_classes=self.IGNORED_CLASS)

        new_point_cloud = o3d.geometry.PointCloud()
        new_point_cloud.points = o3d.utility.Vector3dVector(xyz_points.T)
        self.pcd += new_point_cloud

        mask_X = np.logical_and(xyz_points[0, :] > self.min_X,
                                xyz_points[0, :] < self.max_X)
        mask_Z = np.logical_and(xyz_points[2, :] > self.min_Z,
                                xyz_points[2, :] < self.max_Z)
        mask_XYZ = np.logical_and.reduce((mask_X, mask_Z))
        xyz_points = xyz_points[:, mask_XYZ]
        sseg_points = sseg_points[mask_XYZ]

        x_coord = np.floor(
            (xyz_points[0, :] - self.min_X) / self.cell_size).astype(int)
        y_coord = np.digitize(xyz_points[1, :], self.y_grid)
        z_coord = (self.H - 1) - np.floor(
            (xyz_points[2, :] - self.min_Z) / self.cell_size).astype(int)

        if x_coord.shape[0] > 0:
            self.four_dim_grid[z_coord, y_coord, x_coord, sseg_points] += 1

            # update the weights for the local map
            self.min_x_coord = min(max(np.min(x_coord) - self.map_boundary, 0),
                                   self.min_x_coord)
            self.max_x_coord = max(
                min(np.max(x_coord) + self.map_boundary, self.W - 1),
                self.max_x_coord)
            self.min_z_coord = min(max(np.min(z_coord) - self.map_boundary, 0),
                                   self.min_z_coord)
            self.max_z_coord = max(
                min(np.max(z_coord) + self.map_boundary, self.H - 1),
                self.max_z_coord)

            self.max_y_coord = max(np.max(y_coord), self.max_y_coord)

        if step_ % self.step_size == 0:
            self.get_semantic_map(step_)

    def get_semantic_map(self, step_):
        """ get the built semantic map. """
        smaller_four_dim_grid = self.four_dim_grid[self.min_z_coord:self.max_z_coord + 1, 0:self.THRESHOLD_HIGH,
                                self.min_x_coord:self.max_x_coord + 1, :]
        # argmax over the category axis
        zyx_grid = np.argmax(smaller_four_dim_grid, axis=3)
        # swap y dim to the last axis
        zxy_grid = np.swapaxes(zyx_grid, 1, 2)
        L, M, N = zxy_grid.shape
        zxy_grid = zxy_grid.reshape(L * M, N)

        semantic_map = find_first_nonzero_elem_per_row(zxy_grid)
        semantic_map = semantic_map.reshape(L, M)
        color_semantic_map = apply_color_to_map(semantic_map)

        if semantic_map.shape[0] > 0:
            self.save_sem_map_through_plt(
                color_semantic_map,
                f'{self.saved_folder}/step_{step_}_semantic.jpg')

    def save_sem_map_through_plt(self, img, name):
        """ save the figure img at directory 'name' using matplotlib"""
        fig, ax = plt.subplots(nrows=1, ncols=1)
        ax.imshow(img)
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)
        fig.tight_layout()
        fig.savefig(name)
        plt.close()

    def save_final_map(self, ENLARGE_SIZE=5):
        """ save the built semantic map to a figure."""
        smaller_four_dim_grid = self.four_dim_grid[self.min_z_coord:self.max_z_coord + 1, 0:self.THRESHOLD_HIGH,
                                self.min_x_coord:self.max_x_coord + 1, :]
        # argmax over the category axis
        zyx_grid = np.argmax(smaller_four_dim_grid, axis=3)
        # swap y dim to the last axis
        zxy_grid = np.swapaxes(zyx_grid, 1, 2)
        L, M, N = zxy_grid.shape
        zxy_grid = zxy_grid.reshape(L * M, N)

        semantic_map = find_first_nonzero_elem_per_row(zxy_grid)
        semantic_map = semantic_map.reshape(L, M)

        map_dict = {}
        map_dict['min_x'] = self.min_x_coord
        map_dict['max_x'] = self.max_x_coord
        map_dict['min_z'] = self.min_z_coord
        map_dict['max_z'] = self.max_z_coord
        map_dict['min_X'] = self.min_X
        map_dict['max_X'] = self.max_X
        map_dict['min_Z'] = self.min_Z
        map_dict['max_Z'] = self.max_Z
        map_dict['W'] = self.W
        map_dict['H'] = self.H
        map_dict['semantic_map'] = semantic_map
        print(f'semantic_map.shape = {semantic_map.shape}')
        np.save(f'{self.saved_folder}/BEV_semantic_map.npy', map_dict)

        semantic_map = cv2.resize(
            semantic_map,
            (int(semantic_map.shape[1] * ENLARGE_SIZE),
             int(semantic_map.shape[0] * ENLARGE_SIZE)),
            interpolation=cv2.INTER_NEAREST)
        color_semantic_map = apply_color_to_map(semantic_map)
        self.save_sem_map_through_plt(color_semantic_map,
                                 f'{self.saved_folder}/final_semantic_map.jpg')
